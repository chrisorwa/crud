% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/confmat.R
\name{confmat}
\alias{confmat}
\title{Confusion matrix}
\usage{
confmat(trues, preds)
}
\arguments{
\item{trues}{actual outcomes}

\item{preds}{predicted outcomes}
}
\value{
conf confusion matrix and stats
}
\description{
Present accuracy statistics of trues against predictions.
Based on http://blog.revolutionanalytics.com/2016/03/com_class_eval_metrics_r.html
}
\details{
acc = accuracy = (TP + TN) / total

err = error rate = 1 - accuracy

tpr = true positive rate = TP / (TP + FN)

tnr = true negative rate = TN / (TN + FP)

fpr = false positive rate = FP / (FP + TN)

fnr = false negative rate = FN / (FN + TN)

ppv = positive predictive value = TP / (TP + FP)

npv = negative predictive value = TN / (TN + FN)


Synonyms:
true positive rate: {sensitivity, recall}

true negative rate: {specificity}

false positive rate: {fallout}

false negative rate: {miss}

positive predictive value: {precision}
}
\examples{
set.seed(0)
actual = c('a','b','c')[runif(100, 1,4)] # actual labels
predicted = actual # predicted labels
predicted[runif(30,1,100)] = actual[runif(30,1,100)]  # introduce incorrect predictions
cm = as.matrix(table(Actual = actual, Predicted = predicted)) # create the confusion matrix
conf(actual, predicted)
}

